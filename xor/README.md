Use a neural net to approximate XOR operation.  

## Result  
<img src="https://github.com/borgwang/toys/raw/master/xor/res/nn.png" width = "240" height = "200" alt="origin" align=center />   2-layer neural net with activation function (ReLU)  
<img src="https://github.com/borgwang/toys/raw/master/xor/res/perceptron.png" width = "240" height = "200" alt="paint" align=center />  Perceptron  

## Notes
A perceptron is essentially a single layer neural net without activation function.  
It has limited learning ability and can only handle data which is linear separable.(as you can see in figure2 above)  
A neural net with activation function can make non-linear transformation in contrast.  
